{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "\n",
    "In this notebook, we will:\n",
    "- Install required and update third-party libraries\n",
    "- Load our dataset into an S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -Uq pip\n",
    "!python -m pip install -Uq sagemaker boto3 awswrangler\n",
    "!python -m pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import pandas as pd  # noqa: E402\n",
    "import geopandas as gpd  # noqa: E402\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader, S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "bucket=sagemaker.Session().default_bucket()\n",
    "prefix = 'sagemaker/DEMO-xgboost-tripfare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive ../glue/ s3://$bucket/scripts/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source = f's3://{bucket}/{prefix}/input/'\n",
    "input_data = input_source + 'data'\n",
    "input_zones = input_source + 'zones/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store input_source\n",
    "%store input_zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset into S3\n",
    "\n",
    "In the following section, we will load the new york trip data and taxi zone data into a predefined S3 bucket/folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive 's3://nyc-tlc/trip data/' $input_data/green --exclude '*' --include 'green_tripdata_2018-1*'\n",
    "!aws s3 cp --recursive 's3://nyc-tlc/trip data/' $input_data/yellow --exclude '*' --include 'yellow_tripdata_2018-1*'\n",
    "!aws s3 cp 's3://nyc-tlc/misc/taxi_zones.zip' $input_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_dir = os.path.join('.', \"input/zones\")\n",
    "zones_file = os.path.join(zones_dir, \"taxi_zones.zip\")\n",
    "zones_file_csv = os.path.join(zones_dir, \"taxi_zones.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trip data and taxi zones to input folder\n",
    "download_uri = \"s3://nyc-tlc/misc/taxi_zones.zip\"\n",
    "S3Downloader().download(download_uri, zones_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(zones_file, \"r\") as zip:\n",
    "    zip.extractall(zones_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the zone file and convert it to csv\n",
    "zone_df = gpd.read_file(os.path.join(zones_dir, \"taxi_zones.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centroids as EPSG code of 3310 to measure distance\n",
    "zone_df[\"centroid\"] = zone_df.geometry.centroid.to_crs(epsg=3310)\n",
    "# Convert cordinates to the WSG84 lat/long CRS has a EPSG code of 4326.\n",
    "zone_df[\"latitude\"] = zone_df.centroid.to_crs(epsg=4326).x\n",
    "zone_df[\"longitude\"] = zone_df.centroid.to_crs(epsg=4326).y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_df.to_csv(zones_file_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file to s3\n",
    "S3Uploader().upload(zones_file_csv, input_source + 'zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $input_data/green/\n",
    "!aws s3 ls $input_data/yellow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded our data, take note of the S3 bucket for the two dataset which we will be using\n",
    "\n",
    "- Input source containing our taxi trip data\n",
    "- Input zones containing taxi zones which we will use to join with our taxi trip data to enrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input source\", input_source)\n",
    "print(\"Input zones\", input_zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
